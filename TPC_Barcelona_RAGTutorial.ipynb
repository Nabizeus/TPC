{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nabizeus/TPC/blob/main/TPC_Barcelona_RAGTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpjFGh2qX2O"
      },
      "source": [
        "#Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACA8jfiCDuOU"
      },
      "source": [
        "## Overview\n",
        "*   Motivation for RAG\n",
        "*   Idea behind RAG\n",
        "*   Advantages and Disadvantages\n",
        "*   Implementation to augment question + answer\n",
        "*   Advanced applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5YjAA14ih8R"
      },
      "source": [
        "#### Imagine you went to live under a rock on August 2006. When you come out in 2024, you are asked how many planets revolve around the sun. What would you say?...\n",
        "![pluto](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/pluto_planets.jpeg?raw=1)\n",
        "\n",
        "This is similar to LLMs which are trained with data until a certain point and then asked questions on data they are not trained on. Understandably, LLMs will either be unable to answer or simply hallucinate a probably wrong answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3HmhY-rl29V"
      },
      "source": [
        "###What can be done?\n",
        "\n",
        "Have the LLM go to the library using **Research Augmented Generation (RAG)**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXtqMcK1nAv_"
      },
      "source": [
        "RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGt5yQVrz0C"
      },
      "source": [
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag-overview.original.png?raw=1)\n",
        "Image credit: https://scriv.ai/guides/retrieval-augmented-generation-overview/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6N4A87lt3Wq"
      },
      "source": [
        "RAG has been shown to improve LLM prediction accuracy without needing to increase parameter size.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_acc_v_size.png?raw=1)\n",
        "\n",
        "*Image credit: Yu, Wenhao. \"Retrieval-augmented generation across heterogeneous knowledge.\" Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 2022.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDcwB4BMyUQ5"
      },
      "source": [
        "RAG also increases explainability by giving the source for information.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_source_locator.png?raw=1)\n",
        "\n",
        "Image credit: https://ai.stanford.edu/blog/retrieval-based-NLP/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUJ2eQvJIAg"
      },
      "source": [
        "## Advantages and Disadvantages\n",
        "\n",
        "### Advantages\n",
        "\n",
        "*   Provides domain specific context\n",
        "*   Improves predictive performance and reduces hallucinations\n",
        "*   Does not increase model parameters\n",
        "*   Less labor intensive than fine-tuning LLMs\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "*   May introduce latency since we are adding a relatively costly search step\n",
        "*   If your dataset includes private information, you may inadvertently expose another user with this information.\n",
        "*   The data you want to use needs to be curated and you should decide how the data should be accessed. This adds time for the initial set-up.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Iqfmjy-1E"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVkTgnU80jjV"
      },
      "source": [
        "### 1. Install + load relevant modules:\n",
        "*   langchain\n",
        "*   torch\n",
        "*   transformers\n",
        "*   sentence-transformers\n",
        "*   datasets\n",
        "*   faiss-cpu  \n",
        "*   pypdf\n",
        "*  unstructure[pdf]\n",
        "*  huggingface_hub (add hf_token)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c1uDuyDLd1Xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3391fc8e-85a3-49c7-a10f-e2505b6f88a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.1.5\n",
            "  Using cached langchain-0.1.5-py3-none-any.whl (806 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain==0.1.5)\n",
            "  Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "Collecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.5)\n",
            "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.5)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.5) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (3.0.0)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain==0.1.5)\n",
            "  Using cached langchain_community-0.0.37-py3-none-any.whl (2.0 MB)\n",
            "  Using cached langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "  Using cached langchain_community-0.0.35-py3-none-any.whl (2.0 MB)\n",
            "  Using cached langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "  Using cached langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
            "  Using cached langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "  Using cached langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_community-0.0.30-py3-none-any.whl (1.9 MB)\n",
            "  Using cached langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
            "  Using cached langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "  Using cached langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "  Using cached langchain_community-0.0.26-py3-none-any.whl (1.8 MB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
            "  Using cached langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
            "  Using cached langchain_community-0.0.23-py3-none-any.whl (1.7 MB)\n",
            "  Using cached langchain_community-0.0.22-py3-none-any.whl (1.7 MB)\n",
            "  Using cached langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n",
            "  Using cached langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.5)\n",
            "  Using cached langchain_core-0.1.51-py3-none-any.whl (302 kB)\n",
            "  Using cached langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "  Using cached langchain_core-0.1.49-py3-none-any.whl (303 kB)\n",
            "  Using cached langchain_core-0.1.48-py3-none-any.whl (302 kB)\n",
            "  Using cached langchain_core-0.1.47-py3-none-any.whl (302 kB)\n",
            "  Using cached langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
            "  Using cached langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
            "  Using cached langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
            "  Using cached langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
            "  Using cached langchain_core-0.1.41-py3-none-any.whl (278 kB)\n",
            "  Using cached langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_core-0.1.39-py3-none-any.whl (276 kB)\n",
            "  Using cached langchain_core-0.1.38-py3-none-any.whl (279 kB)\n",
            "  Using cached langchain_core-0.1.37-py3-none-any.whl (274 kB)\n",
            "  Using cached langchain_core-0.1.36-py3-none-any.whl (273 kB)\n",
            "  Using cached langchain_core-0.1.35-py3-none-any.whl (273 kB)\n",
            "  Using cached langchain_core-0.1.34-py3-none-any.whl (271 kB)\n",
            "  Using cached langchain_core-0.1.33-py3-none-any.whl (269 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.7.1)\n",
            "  Using cached langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "  Using cached langchain_core-0.1.31-py3-none-any.whl (258 kB)\n",
            "  Using cached langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "  Using cached langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
            "  Using cached langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
            "  Using cached langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
            "  Using cached langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
            "  Using cached langchain_core-0.1.25-py3-none-any.whl (242 kB)\n",
            "  Using cached langchain_core-0.1.24-py3-none-any.whl (241 kB)\n",
            "  Using cached langchain_core-0.1.23-py3-none-any.whl (241 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain==0.1.5)\n",
            "  Using cached langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.5) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.5) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.5) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (1.0.0)\n",
            "Installing collected packages: langsmith, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.1.79\n",
            "    Uninstalling langsmith-0.1.79:\n",
            "      Successfully uninstalled langsmith-0.1.79\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.9\n",
            "    Uninstalling langchain-core-0.2.9:\n",
            "      Successfully uninstalled langchain-core-0.2.9\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.5\n",
            "    Uninstalling langchain-community-0.2.5:\n",
            "      Successfully uninstalled langchain-community-0.2.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.5\n",
            "    Uninstalling langchain-0.2.5:\n",
            "      Successfully uninstalled langchain-0.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-experimental 0.0.61 requires langchain-community<0.3.0,>=0.2.5, but you have langchain-community 0.0.20 which is incompatible.\n",
            "langchain-experimental 0.0.61 requires langchain-core<0.3.0,>=0.2.7, but you have langchain-core 0.1.23 which is incompatible.\n",
            "langchain-text-splitters 0.2.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.1.5 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: unstructured==0.12.3 in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (0.23.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.12.3) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (2024.6.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (3.3.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (0.27.0)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (3.7)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (3.21.3)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured==0.12.3) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.12.3) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.12.3) (4.66.4)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client>=0.15.1->unstructured==0.12.3) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured==0.12.3) (1.2.1)\n",
            "Requirement already satisfied: unstructured[pdf]==0.12.3 in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (4.12.2)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.23.7)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.14.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.16.1)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (20231228)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (9.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (4.2.0)\n",
            "Requirement already satisfied: unstructured-inference==0.7.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.7.23)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]==0.12.3) (0.3.12)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.23.3)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.8.0.76)\n",
            "Requirement already satisfied: onnxruntime<1.16 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.15.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.41.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (2024.6.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (3.3.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (0.27.0)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (3.7)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.0.6)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (3.21.3)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.0.0)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (2.0.7)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]==0.12.3) (10.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]==0.12.3) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]==0.12.3) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]==0.12.3) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]==0.12.3) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]==0.12.3) (4.66.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[pdf]==0.12.3) (3.20.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]==0.12.3) (42.0.8)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[pdf]==0.12.3) (1.2.14)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.12.3) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (0.14.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2023.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.0.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.11.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.18.0+cu121)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.4.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.3.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.12.3) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client>=0.15.1->unstructured[pdf]==0.12.3) (1.2.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.0.3)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (12.5.40)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2024.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.30.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[pdf]==0.12.3) (3.1.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.1.5\n",
        "!pip install --quiet langchain_experimental\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install unstructured==0.12.3\n",
        "!pip install unstructured[pdf]==0.12.3\n",
        "!pip install tiktoken\n",
        "!pip install huggingface_hub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8YpJK93pNsZ",
        "outputId": "35ccfe3f-ca50-404a-9b80-c3923ee48fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-workshop'...\n",
            "remote: Enumerating objects: 273, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 273 (delta 64), reused 88 (delta 55), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (273/273), 44.89 MiB | 30.32 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n",
            "--2024-06-18 21:56:58--  https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/e1c8c4fcfc48f347033239c8a023403d_6-00F08-L10.pdf\n",
            "Resolving ocw.mit.edu (ocw.mit.edu)... 151.101.194.133, 151.101.130.133, 151.101.66.133, ...\n",
            "Connecting to ocw.mit.edu (ocw.mit.edu)|151.101.194.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135352 (132K) [application/pdf]\n",
            "Saving to: e1c8c4fcfc48f347033239c8a023403d_6-00F08-L10.pdf\n",
            "\n",
            "e1c8c4fcfc48f347033 100%[===================>] 132.18K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-06-18 21:56:59 (6.64 MB/s) - e1c8c4fcfc48f347033239c8a023403d_6-00F08-L10.pdf saved [135352/135352]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download supporting data from llm-workshop + MIT Opencourseware\n",
        "\n",
        "!git clone https://github.com/argonne-lcf/llm-workshop.git\n",
        "!wget https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/e1c8c4fcfc48f347033239c8a023403d_6-00F08-L10.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7LNmxVcRCOp",
        "outputId": "80110060-ad1b-4a17-f14e-8e307d2972d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter huggingfacehub api token: \n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Create a HF token key from https://huggingface.co/settings/tokens so that you\n",
        "# can login to HF from inside this notebook\n",
        "from huggingface_hub import login\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass('Enter huggingfacehub api token: ')\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agp6iRm-7jD"
      },
      "source": [
        "### 2. Choose a dataset to use and then load it into your code\n",
        "Here we are using the pdfs loaded in pdfs/. We load this using langchain DirectoryLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBx0cQJ9Jmtv"
      },
      "source": [
        "We can load multiple types of datasets into this example though the most commonly used are PDFs and websites.\n",
        "\n",
        "To load websites, we could also use `langchain WebBaseLoader`\n",
        "\n",
        "In this example, we will consider PDFs and load them in using `langchain DirectoryLoader`.\n",
        "\n",
        "We host all PDFs at the PDFs directory `llm-workshop/tutorials/04-rag/PDFs`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYoEp4FJz6oK",
        "outputId": "a747f322-7fa8-4301-93c5-6633ff62c655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 7/7 [00:28<00:00,  4.03s/it]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('llm-workshop/tutorials/04-rag/PDFs', glob=\"**/*.pdf\", show_progress=True)\n",
        "papers = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvyEOMMBB7G"
      },
      "source": [
        "### 3. Now, we need to split our documents into chunks.\n",
        "We want the embedding to be greater than 1 word but much less than an entire page. This is essential for the similarity search between the query and the document. Essentially, the query will be searched for greatest similarity to embedded chunks in the dataset. Then those chunks with greatest similarity are augmented to the query.\n",
        "\n",
        "It is essential to choose the chunking method according to your data type.\n",
        "There are different ways to do this:\n",
        "\n",
        "Fixed size\n",
        "*   Token: Splits text on tokens. Can chunk tokens together\n",
        "*   Character: Splits based on some user defined character.\n",
        "\n",
        "Recursive\n",
        "*  Recursively splits text. Useful for keeping related pieces of text next to each other.\n",
        "\n",
        "Document based\n",
        "*   HTML: Splits text based on HTML-specific characters.\n",
        "*   Markdown: Splits on Markdown-specific characters\n",
        "*   Code: Splits text based on characters specific to coding languages.\n",
        "\n",
        "Semantic chunking\n",
        "*   Extract semantic meaning from embeddings and then assess the semantic relationship between these chunks. Essentially splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.\n",
        "\n",
        "Here we use recursive where the dataset is split using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].  A large text is split by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. This continues until the chunk size is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfgRcG6g0XOG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "character_chunker = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\"\\n\\n\"])\n",
        "char_chunks = character_chunker.split_documents(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBELhDegRTZ-",
        "outputId": "b772ccdd-e157-448b-89dd-4be9a3b75288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='7 1 0 2\\n\\ny a M 7 1\\n\\n]\\n\\nG L . s c [\\n\\n2 v 6 7 0 7 0 . 3 0 7 1 : v i X r a\\n\\nSMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules\\n\\nEsben Jannik Bjerrum*\\n\\nWildcard Pharmaceutical Consulting, Frdings All 41, 2860 Sborg, Denmark *) esben@wildcardconsulting.dk\\n\\nAbstract' metadata={'source': 'llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf'} \n",
            "\n",
            "page_content='\\n\\nSimplied Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been dened, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coecient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coecient of 0.68 and a RMS of 0.52 was found.' metadata={'source': 'llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf'} \n",
            "\n",
            "page_content='Introduction\\n\\nshown Neural networks and deep learning has image such as interesting application successes, classication[1], and speech recognition[2]. One of the issues that limits their general applicability in the QSAR domain may be the limited sizes of the labeled datasets available, although successes do appear.[3] Limited datasets necessitates harsh regularization or shallow and narrow architectures. Within image anal- ysis and classication, data augmentation techniques has been used with excellent results.[4, 5, 6, 7] As an example, a dataset of labeled images can be enlarged by operations such as mirroring, rotation, morphing and zooming. The afterwards trained network gets more robust towards such variations and the neural network can recognize the same object in dierent versions.' metadata={'source': 'llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf'} \n",
            "\n",
            "page_content='Neural networks has also been used on molec- ular data, where the input may be calculated descriptors,[3] neural network interpretation of the molecular graph[8] or also SMILES representations.[9] Simplied Molecular Input Line Entry System (SMILES) is a single line text based molecular nota- tion format.[10] A single molecule has multiple pos- sible SMILES strings, which has led to the deni- tion of a canonical SMILES,[11] which ensures that a molecule corresponds to a single SMILES string. The possibilities for variation in the SMILES strings of simple molecules are limited. Propane has two possibilities CCC and C(C)C. But as the molecule gets larger in size and more complex in branching,\\n\\nTolueneSMILESEnumerationCc1ccccc1c1ccccc1Cc1(C)ccccc1c1c(C)cccc1c1cc(C)ccc1c1ccc(C)cc1c1cccc(C)c1' metadata={'source': 'llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf'} \n",
            "\n",
            "page_content='Figure 1: SMILES enumeration enables data augmen- tation. The molecule toluene corresponds to seven dif- ferent SMILES, the top one is the canonical smile. One data point with toluene in the dataset would thus leads to seven samples in the augmented dataset.\\n\\nthe number of possible SMILES strings grows rapidly. Toluene with seven atoms, has seven possible SMILES strings (Figure 1).\\n\\nHere data augmentation of molecular structures with SMILES enumeration for QSAR studies will be investigated using long short term memory (LSTM) cell neural networks inspired by networks used for Twitter tweets sentiment analysis.[12]\\n\\n1\\n\\nMethods\\n\\nSMILES enumeration' metadata={'source': 'llm-workshop/tutorials/04-rag/PDFs/1703.07076.pdf'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in char_chunks[0:5]:\n",
        "  print(i, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcUt5rgsRo69",
        "outputId": "11f11d36-33d9-405b-cfa2-f3f85aba04ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 papers have been split into 361 chunks.\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(papers)} papers have been split into {len(char_chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib00jI51bUIJ"
      },
      "source": [
        "#### Example: Comparing Naive Chunking with Semantic Chunking\n",
        "\n",
        "Using a lecture transcript from MIT OpenCourseware on [Binary Trees: Fall 2008 Lecture 10](https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/resources/6-00f08-l10/) we can see the difference between naive chunking and semantic chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve1TJoPGjY3q"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"e1c8c4fcfc48f347033239c8a023403d_6-00F08-L10.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# First page of lecture is liscence, ignore and get text for all other pages\n",
        "lecture_text = \"\".join(elem.page_content for elem in pages[1:])\n",
        "\n",
        "lecture =  Document(page_content=lecture_text, metadata={\"source\": \"local\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogYJBUDxbegf",
        "outputId": "da47bdec-fe3e-42dc-d4a7-f85914451a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Initialize the encoder model.\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "model_name = \"sentence-transformers/msmarco-distilbert-dot-v5\"\n",
        "model_kwargs = {'device':'cuda'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "\n",
        "encoder = HuggingFaceEmbeddings(\n",
        "  model_name = model_name,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "#Perform semantic chunking.\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "#initializing the splliter.\n",
        "semantic_chunker = SemanticChunker(encoder, buffer_size=5)\n",
        "\n",
        "#list of grouped_sentences (buffers)\n",
        "buffers = semantic_chunker.split_documents([lecture])\n",
        "buffers = [buffer.page_content for buffer in buffers]\n",
        "semantic_chunks = semantic_chunker.create_documents(buffers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcr_mLCVkX2a"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "character_chunker = CharacterTextSplitter(chunk_size=500, chunk_overlap=150, separator=\" \")\n",
        "char_chunks = character_chunker.split_documents([lecture])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbC1xF_Mc5Ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24c32a3-2eea-4363-8109-d2850678a807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic chunking\n",
            "page_content=\"We talked about things you could do to try make sure that happens. You could run \\nthrough a little loop to say keep trying until you get one. But one of the ways I could \\ndeal with it is what's shown here. And what's this little loop say to do? This little loop \\nsays I'm going to write a function or procedures that takes in two messages. I'm going to run through a loop, and I'm going to request some input, which I'm \\ngoing to read in with raw input. I'm goi ng to store that into val. And as you might \\nexpect, I'm going to then try and see if I can convert that into a float. Oh wait a \\nminute, that's a little different than what we did last time, right?\" \n",
            "\n",
            "page_content=\"STUDENT: [UNINTELLIGIBLE] \\nPROFESSOR: In both lists, right. So this is linear, order n and n is this sum of the \\nelement, or sorry, the number of elements in each list. I said I was going to back my \\nway into this. That gives me a way to merge things. So here's what merge sort \\nwould do. Merge sort takes this idea of divide and conquer, and it does the following: it says \\nlet's divide the list in half. There's the divide and conquer. And let's keep dividing \\neach of those lists in half until we get down to something that's really easy to sort. What's the simplest thing to sort? A list of size one, right? So continue until we have \\nsingleton lists. Once I got a list of size one they're sorted, and then combine them. Combine them \\nby doing emerge the sub-lists. And again, you see that flavor.\" \n",
            "\n",
            "page_content=\"I'm \\ncreating a list with just that special symbol none in it.\" \n",
            "\n",
            "page_content=\"Last time we \\nchecked the type and said if it is a float you're okay.\" \n",
            "\n",
            "page_content=\"Well that's simple. Given that representation and some value, I just say gee is it \\nthere? What's the order complexity here? I know I drive you nuts asking questions? What's the order complexity here? Quadratic, linear, log, constant? Any takers? I \\nknow I have the wrong glasses on the see hands up too, but... STUDENT: [UNINTELLIGIBLE] \\nPROFESSOR: Who said it?\" \n",
            "\n",
            "page_content='STUDENT: Constant.' \n",
            "\n",
            "page_content=\"The obvious \\none is to divide it in half, but there may be cases where there are different divisions \\nyou want to have take place. The second question I want to ask is what's the base case? When do I get down to a \\nproblem that's small enough that it's basically trivial to solve? Here it was lists of size \\none. I could have stopped at lists of size two right. That's an easy comparison. Do \\none comparison and return one of two possible orders on it, but I need to decide \\nthat.\" \n",
            "\n",
            "page_content=\"But jumping ahead, I'm going to skip in a second now to talk \\nabout one last linguistic thing from Python, but I want to preface Professor Guttag is \\ngoing to pick up next week, and what we're going to start doing then is taking these \\nclasses of algorithms and start looking at much more complex algorithms.\" \n",
            "\n",
            "page_content='We said if we had an ordered list, we could use binary \\nsearch.' \n",
            "\n",
            "page_content=\"If you \\nreally know what they are use them as search, but if you think there's going to be \\nsome flexibility, you want to prevent the user getting trapped in a bad spot, and \\nexceptions as a consequence are a good thing to use.\" \n",
            "\n",
            "\n",
            "\n",
            " ----------- \n",
            "\n",
            "\n",
            "Fixed-size character chunking\n",
            "page_content=\"the list. I'm \\nreturning that as my set. And then to create the object, I'll simply do a set of inserts. \\nIf I want the values two, six and eight in there, I would do an insert of two into that \\nset, an insert of six into that set, and an insert of eight into the set. And what does it \\ndo? It marks a one in each of those spots. \\nNow, what did I want to do? I wanted to check membership. I want to do search. \\nWell that's simple. Given that representation and some value, I just say gee is it\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"of strings, well you \\nbasically just generalize the hash function. I think one of the classic ones for strings \\nis called the Rabin-Karp algorithm. And it's simply the same idea that you have a \\nmapping from your import into a set of integers. \\nWow, OK, maybe not so wow, but this is now constant. This is constant time access. \\nSo I can do searching in constant time which is great. Where's the penalty? What did \\nI trade off here? Well I'm going to suggest that what I did was I really traded space\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"happening is that we're typing in something, an expression it \\ndoesn't know how to deal. It's raising the exception, but is this simply bubbl ing up at \\nthe top level saying you've got a problem. \\nSuppose instead you're in the middle of some deep piece of code and you get one of \\nthese cases. It's kind of annoying if it throws it all the way back up to top level for \\nyou to fix. If it's truly a bug, that's the right thing to do. You want to catch it. But in \\nmany cases exceptions or things that,\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"the problem simpler, \\nbut the combination turns out to be really complex, I've not gained anything. \\nSo things that are good candidates for divide and conquer are problems where it's \\neasy to figure out how to divide down, and the combination is of little complexity. It \\nwould be nice if it was less than linear, but linear is nice because then I'm going to \\nget that n log in kind of behavior. And if you ask the TAs in recitation tomorrow, \\nthey'll tell you that you see a lot of n log n\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"come back in a second to help binary searches matches in that, but that's \\nwhat we're going to do. For each of those sub-problems we're going to solve them \\nindependently, and then we're going to combine those solutions. \\nAnd it's called divide and conquer for the obvious reason. I'm going to divide it up \\ninto sub-problems with the hope that those sub-problems get easier. It's going to be \\neasier to conquer if you like, and then I'm going to merge them back. Now, in the \\nbinary search case, in\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"property. That looks like an n log n algorithm \\nbecause it has a particular property. \\nAnd the third thing we've done is we've given you now a set of sort of standard \\nalgorithms if you like. Root force, just walk through every possible case. It works \\nwell if the problem sizes are small. We've had, there are a number of variants of \\nguess and check or hypothesize and test, where you try to guess the solution and \\nthen check it and use that to refine your search.Successive approximation,\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"gives me a float, I'm set, But if not, I'd like to have \\nthe code handle the exception. And that's what this funky tri-accept thing does. This \\nis a tri-accept block and here's the flow of control that takes place in there. \\nWhen I hit a tri-block. It's going to literally do that. It's going to try and execute the \\ninstructions. If it can successfully execute the instructions, it's going to skip past the \\nexcept block and just carry on with the rest of the code. If, however, it raises an\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"bi-section, which is really \\njust a very difficult of successive approximation, but divide and conquer is a class of \\nalgorithm. These are tools that you want in your tool box. These are the kinds of \\nalgorithms that you should be able to recognize. And what I'd like you to begin to do \\nis to look at a problem and say, gee, which kind of algorithm is most likely to be \\nsuccessful on this problem, and map it into that case. \\nOK, starting next -- don't worry I'm not going to quit 36 minutes after\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"case, I got to do order n things k times. It's order k n. \\nWhereas in the ordered case, I need to get them sorted, which is still n log n, but \\nthen the search is only log n. I need to do k of those. And we suggested well this is \\nbetter than that. This is certainly better than that. m plus k all times log n is in \\ngeneral going to be much better than k times n. It depends on n and k but obviously \\nas n gets big, that one is going to be better. \\nAnd that's just a way of reminding you that we\" metadata={'source': 'local'} \n",
            "\n",
            "page_content=\"case \\nthey can handle it, but it's a particular kind of exception. \\nI might do something like, remind you I have test. If I do this, try and get the 10th \\nelement of a list that's only eight long. I get what looks like an error, but it's actually \\nthrowing an exception. The exception is right there. It's an index error, that is it's \\ntrying to do something going beyond the range of what this thing could deal with. \\nOK, you say, come on, I've seen these all the time. Every time I type something\" metadata={'source': 'local'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "random.shuffle(semantic_chunks)\n",
        "random.shuffle(char_chunks)\n",
        "\n",
        "print(\"Semantic chunking\")\n",
        "for i in semantic_chunks[0:10]:\n",
        "  print(i, \"\\n\")\n",
        "\n",
        "print(\"\\n\\n ----------- \\n\\n\")\n",
        "\n",
        "print(\"Fixed-size character chunking\")\n",
        "for i in char_chunks[0:10]:\n",
        "  print(i, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06M-jDjBmsQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161603f7-1301-43ee-9fe1-4d31f9d38f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks produced by semantic chunking: 64\n",
            "Number of chunks produced by character chunking: 119\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of chunks produced by semantic chunking: {len(semantic_chunks)}\")\n",
        "print(f\"Number of chunks produced by character chunking: {len(char_chunks)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InCjG0vln9Aj"
      },
      "source": [
        "#### ProTip: Semantic Chunking is not suitable to poorly-parsed PDF contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOrKjSCtoMbf"
      },
      "outputs": [],
      "source": [
        "#Initialize the biomedical domain-specific encoder model.\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "model_name = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
        "model_kwargs = {'device':'cuda'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "\n",
        "biomedical_encoder = HuggingFaceEmbeddings(\n",
        "  model_name = model_name,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOCrOZ1nrGBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5995b61-ebc4-4eb7-c072-b7d879a7f426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='7 1 0 2\\n\\ny a M 7 1\\n\\n]\\n\\nG L . s c [\\n\\n2 v 6 7 0 7 0 . 3 0 7 1 : v i X r a\\n\\nSMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules\\n\\nEsben Jannik Bjerrum*\\n\\nWildcard Pharmaceutical Consulting, Frdings All 41, 2860 Sborg, Denmark *) esben@wildcardconsulting.dk\\n\\nAbstract\\n\\nSimplied Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been dened, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coecient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coecient of 0.68 and a RMS of 0.52 was found. Introduction\\n\\nshown Neural networks and deep learning has image such as interesting application successes, classication[1], and speech recognition[2]. One of the issues that limits their general applicability in the QSAR domain may be the limited sizes of the labeled datasets available, although successes do appear.[3] Limited datasets necessitates harsh regularization or shallow and narrow architectures. Within image anal- ysis and classication, data augmentation techniques has been used with excellent results.[4, 5, 6, 7] As an example, a dataset of labeled images can be enlarged by operations such as mirroring, rotation, morphing and zooming. The afterwards trained network gets more robust towards such variations and the neural network can recognize the same object in dierent versions. Neural networks has also been used on molec- ular data, where the input may be calculated descriptors,[3] neural network interpretation of the molecular graph[8] or also SMILES representations.[9] Simplied Molecular Input Line Entry System (SMILES) is a single line text based molecular nota- tion format.[10] A single molecule has multiple pos- sible SMILES strings, which has led to the deni- tion of a canonical SMILES,[11] which ensures that a molecule corresponds to a single SMILES string. The possibilities for variation in the SMILES strings of simple molecules are limited. Propane has two possibilities CCC and C(C)C. But as the molecule gets larger in size and more complex in branching,\\n\\nTolueneSMILESEnumerationCc1ccccc1c1ccccc1Cc1(C)ccccc1c1c(C)cccc1c1cc(C)ccc1c1ccc(C)cc1c1cccc(C)c1\\n\\nFigure 1: SMILES enumeration enables data augmen- tation. The molecule toluene corresponds to seven dif- ferent SMILES, the top one is the canonical smile. One data point with toluene in the dataset would thus leads to seven samples in the augmented dataset. the number of possible SMILES strings grows rapidly. Toluene with seven atoms, has seven possible SMILES strings (Figure 1). Here data augmentation of molecular structures with SMILES enumeration for QSAR studies will be investigated using long short term memory (LSTM) cell neural networks inspired by networks used for Twitter tweets sentiment analysis.[12]\\n\\n1\\n\\nMethods\\n\\nSMILES enumeration\\n\\nSMILES enumeration was done with a Python script utilizing the cheminformatics library RDKit.[13] The atom ordering of the molecule is scrambled ran- domly by converting to molle format[14] and chang- ing the atom order, before converting back to the RDKit mol A SMILES is then gener- ated using RDKit with the default option of pro- ducing canonical SMILES set to false, where dif- ferent atom orderings lead to dierent SMILES. The SMILES strings is then compared and possible added to a growing set of unique SMILES strings. The process is repeated a predened number of times. The python functions are available on github: https://github.com/Ebjerrum/SMILES-enumeration\\n\\nformat. Molecular dataset\\n\\nThe dataset was obtained from Sutherland et al 2003.[15] It consists of 756 dihydrofolate inhibitors with P. carinii DHFR inhibition data.' \n",
            "\n",
            "page_content='The dataset was split in test and a training set in a 1:9 ratio. It was expanded with SMILES enumeration and the SMILES strings were padded with spaces to xed length of 74, which is one characters longer than the longest SMILES in the dataset. It was subsequently vectorized by one-hot encoding the characters into a bit matrix with one bit set for the corresponding char- acter in each row using a generated char to int dic- tionary. Molecules where the associated anity was not a number were removed. The associated IC50 data was converted to log IC50 and normalized to unit variance and mean zero with utilities from Scikit- learn.[16]\\n\\nLSTM neural network\\n\\nTwo dierent neural networks were built and trained using Keras version 1.1.2[17] with Theano v.' \n",
            "\n",
            "page_content='0.8.0[18] as back end. One or more LSTM layers were used in batch mode, and the nal state fed to a feed-forward neural network with a single linear output neuron. The network layout was optimized using Bayesian op- timization with Gaussian processes as implemented in the Python package GpyOpt[19] version 1.0.3, vary- ing the hyper parameters listed in Table 1. 10 ini- tial trainings was done before using the GP_MCMC and the EI_MCMC acquisition function to sample new hyper parameter sets.[20] One network was op- timized and trained only using a dataset with canon- ical SMILES, whereas the other were optimized and trained with the dataset that expanded with SMILES enumeration. In the rest of the publication they will be referred to as the canonical model and enumerated model, respectively. All computations and training were done on a Linux workstation (Ubuntu Mate 16.04) with 4 GB of\\n\\n2\\n\\nram, i5-2405S CPU @ 2.50GHz and an Nvidia Geforce GTX1060 graphics card with 6 GB of ram. Results\\n\\nFiltering, splitting and SMILES enumeration re- sulted in a canonical SMILES dataset with 602 train molecules and 71 test molecules, whereas the enumer- ated dataset had 79143 and 9412 rows for train and test, respectively. This corresponds to an augmenta- tion factor of approximately 130. Each molecule had on average 130 alternative SMILES representations. Optimization of the architecture yielded two dier- ent best congurations of hyper parameters, depend- ing on the dataset used. The best hyper parameters found for each dataset are shown in Table 2. The train history is shown in Figure 2. The best neural network trained on the canonical dataset had a loss of 0.44 including regularization penalty and a mean square error of 0.22 and 0.41 for train and test set, respectively. The curves for the training us- ing the canonical dataset are very noisy (Figure 2A). The best neural network trained on the enumerated dataset loss of 0.18 including regularization penalty and a mean square error of 0.09 and 0.30 for train and test set, respectively. The training curve is sig- nicantly less noisy than for the canonical dataset (Figure 2B). Both neural networks were used to predict the IC50 values from the canonical and enumerated datasets, and the scatter plots are shown in Figure 3. The correlation coecients and root mean square deviation (RMS) are tabulated in Table 3. The combination with the worst performance was pre- dicting the test set molecules is using enumerated SMILES neural network model trained on the canon- ical dataset. Which has a correlation coecient of 0.26 and an RMS of 0.84. The bad correlation is clearly visible from Figure 3 plot C. The best per- formance predicting the test set, was seen with the combination of the enumerated model and the enu- merated SMILES. Here the correlation coecient is 0.66 and the RMS 0.55. The two other combinations, canonical model-canonical SMILES and enumerated model, canonical SMILES are close in performance (Table 3).2\\n\\nFigure 4 show a scatter plot of the average predic- tion for each molecule obtained with the enumerated model. The calculated correlation coecient is 0.68 for the test set and the RMS is 0.52. Discussion\\n\\nThe results clearly suggest that SMILES enumera- tion as a data augmentation technique for molecular data has benets. The model trained on canonical data is not able to predict many of the alternative SMILES of the train and test set as is evident for\\n\\nTable 1: Hyper parameter Search Space\\n\\nParameter Number of LSTM layers Number of units in LSTM layers Dropout for input gates (dropout_W) Dropout for recurrent connection (dropout_U) Number of dense hidden layers Hidden layer size Weight regularization on dense layer, L1 Weight regularization on dense layer, L2 Learning rate\\n\\nSearch Space [1,2] [32, 64, 128, 256] 0  0.2 0  0.5 [0,1] [4, 8, 16, 32, 64, 128] 0  0.2 0  0.2 0.05-0.0001\\n\\nType Discrete Discrete Continuous Continuous Discrete Discrete Continuous Continuous Continuous\\n\\nTable 2: Best Hyperparameters found\\n\\nParameter\\n\\nCanonical Model Enumerated Model\\n\\nNumber of LSTM layers Number of units in LSTM layers Dropout for input gates (dropout_W) Dropout for recurrent connections (dropout_U) Number of dense hidden layers Hidden layer size Weight regularization on dense layer, L1 Weight regularization on dense layer, L2 Learning rate\\n\\n1 128 0.0 0.0 0 N/A 0.2 0.2 0.0001\\n\\n1 64 0.19 0.0 0 N/A 0.005 0.01 0.005\\n\\nFigure 2: Training history for the two datasets and neural networks. A: Neural network trained on canonical SMILES shows a noisy curve where the best model has a test loss of 0.41. B: Neural network trained on enumerated SMILES obtains the best model with a test loss of 0.30. Blue lines are the mean square error without regularization penalty, green is loss including regularization penalty and the red line is mean square error on the test set. 3\\n\\nFigure 3: Scatter plots of predicted vs.' \n",
            "\n",
            "page_content='true values.' \n",
            "\n",
            "page_content='Left column shows scatter plots obtained with the model trained on canonical SMILES only. Right column shows predictions with the model trained on enumerated data. Top row is scatter plots with only canonical SMILES and bottom row is predictions of the enumerated dataset. The blue line denotes the perfect correlation (y = x). 4\\n\\nTable 3: Statistics of predicted values, values are for Train/Test set respectively\\n\\nDataset\\n\\nTrain Test Enumerated Train Test\\n\\nCanonical\\n\\nCanonical Model R2 0.78 0.56 0.25 0.26\\n\\nRMS 0.46 0.62 0.88 0.84\\n\\nEnumerated\\n\\nModel R2 0.85 0.63 0.87 0.66\\n\\nRMS 0.39 0.56 0.37 0.55\\n\\nFigure 4: Average of predictions from the enumerated model for each molecule. Train set R2 is 0.88 and RMS is 0.38. Test set R2 is 0.68 and RMS is 0.52. Figure 3 plot C, where the bad generalization to non canonical SMILES strings are evident. Instead the best performance was observed by taking the average for each molecule of the predictions of the enumer- ated SMILES using the enumerated model (Figure 4), which shows that the SMILES enumeration can also be of value in the sampling phase. The canonical model needed a lot more epochs to train, but here it must be considered that the dataset contained 130 times fewer examples. Thus each epoch in the train- ing was only 3 mini batches leading to 3 updates of the weights, whereas the enumerated dataset had ap- proximately 360 updates of the weights of the neural network per epoch. The curves in Figure 2 thus rep- resents 3000 and 18000 updates of the weights. The higher overhead of running more epochs however led to approximately the same wall clock time in train- ing. The hyper parameters found during the opti- mization of the network architecture and amount of regularization was not entirely as expected. The ex- pectation was that the canonical dataset would prefer a smaller and simpler network with a larger regular- ization. Instead the canonical dataset has a larger amount of LSTM cells (128) with no dropout, but a much larger regularization of the nal weights to the input neuron (L1 and L2 maxed out at 0.2). The enu- merated model had fewer LSTM cells (64) and thus fewer connections, but nevertheless found dropout on the input to the LSTM cells to be benecial. To test if\\n\\n5\\n\\nthe dierences were due to the Bayesian optimization getting trapped in a local minimum, the network ar- chitecture found for the enumerated dataset was test trained with the dataset with the canonical SMILES only. The rst try with a learning rate of 0.005 failed (results not shown), but lowering the learning rate to the one found for the canonical SMILES (0.0001), gave a model with a correlation coecient of 0.5 and RMS of 0.68 on the train set. The predictive per- formance was even lower with 0.45 and 0.69, for R2 and RMS respectively. The dierences in hyper pa- rameters after optimization of using the two dierent datasets thus seems justied. The study lacks the division into train, test and validation set, where the hyper parameters are tuned on the test set, but the - nal performance evaluated on the validation set. The observed prediction performance of the LSTM-QSAR models are thus likely overestimated to some degree. However, this study is focused on the gains of us- ing SMILES enumeration and not on producing the optimal DHFR QSAR model. The performance on both the train and test set are lower for the canonical model. If the dierences in performance had been due to to over-tting, the smaller dataset would probably have had an advantage. The use of SMILES as descriptors for QSAR is not new[21, 22, 21] and is as an example implemented in the CORAL software.[23] The approach in the CORAL software is however very dierent from the one in this study. CORAL software breaks down the SMILES into single atoms, double atoms and triple atoms (Sk, SSk and SSSk) as well as some extra man- ually coded extracted features such as BOND, NOSP, HALO and PAIR.[23, 21] The approach seems close to using a mixture of topological torsions[24] with one, two and three atoms and atom-pair[25] ngerprints. The LSTM-QSAR used in this approach directly uses the SMILES string and supposedly let the model best extract the features from the SMILES strings that best t with the task at hand, and similar approaches have been shown to outperform other common ma- chine learning algorithms[22], although the details of optimization of the competing algorithms were not completely clear. SMILES were also used recently in an application of a neural network based auto-encoder.[9] Here the SMILES are used as input to a neural network with the task of recreating the input sequence. The infor- mation is passed through a bottle-neck layer in be- tween the encoder and the decoder, which limits the direct transfer of information. The bottle neck layer thus ends up as a more continuous oating point vec- tor representation of the molecule, which can be used to explore the chemical space near an input molecule, interpolate between molecules and link the vector representation to physico-chemical properties. The amount of unlabeled molecules for the study already surpassed the needed amount, but could in principle be expanded even more with the SMILES enumera-\\n\\ntion technique described here. SMILES enumeration could possible allow the autoencoder to be trained with smaller and more focused datasets of biological interest. Additionally, tt would be interesting to see if dierent SMILES of the same molecule ends up with the same vector representation or in entirely dierent areas in the continuous molecular representations. LSTM networks have also been used in QSAR ap- plications demonstrating learning transfer from large datasets to smaller.[26] Here the input was however not SMILES strings but rather molecular graph con- volution layers[27] working directly on the molecular graph representation. The approach thus more di- rectly reads in the topology of the molecular model, rather than indirectly letting the network infer the topology from the SMILES branching and ring clo- sures dened by the brackets and numbering in the SMILES strings. Conclusion\\n\\nThis short investigation has shown promise in us- ing SMILES enumeration as a data augmentation technique for neural network QSAR models based on SMILES data. SMILES enumeration enables the use of more limited sizes of labeled data sets for use in modeling by more complex neural net- work models. SMILES enumeration gives more robust QSAR models both when predicting sin- gle SMILES, but even more when taking the av- erage prediction using enumerated SMILES for the same molecule. The SMILES enumeration code as well as some of the scripts used for generating the LSTM-QSAR models are available on GitHub: https://github.com/Ebjerrum/SMILES-enumeration\\n\\nConicts of Interest\\n\\nE. J. Bjerrum is the owner of Wildcard Pharmaceu- tical Consulting. The company is usually contracted by biotechnology/pharmaceutical companies to pro- vide third party services\\n\\nReferences\\n\\n[1] P. Y.' \n",
            "\n",
            "page_content='Simard, D.' \n",
            "\n",
            "page_content='Steinkraus, J. C.' \n",
            "\n",
            "page_content='Platt, et al., Best practices for convolutional neural networks applied to visual document analysis., in: ICDAR, Vol. 3, Citeseer, 2003, pp. 958962. [2] G. Hinton, L. Deng, D.' \n",
            "\n",
            "page_content='Yu, G. E. Dahl, A.-r. Mohamed, N.' \n",
            "\n",
            "page_content='Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, et al., Deep neural networks for acoustic modeling in speech recog- nition: The shared views of four research groups, IEEE Signal Processing Magazine 29 (6) (2012) 8297. 6\\n\\n[3] A. Mayr, G. Klambauer, T. Unterthiner, S. Hochreiter, Deeptox: toxicity prediction using deep learning, Frontiers in Environmental Sci- ence 3 (2016) 80.' \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Perform semantic chunking on scientific papers.\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "#initializing the splliter.\n",
        "semantic_chunker = SemanticChunker(biomedical_encoder, buffer_size=5)\n",
        "\n",
        "#list of grouped_sentences (buffers)\n",
        "buffers = semantic_chunker.split_documents(papers)\n",
        "buffers = [buffer.page_content for buffer in buffers]\n",
        "semantic_chunks = semantic_chunker.create_documents(buffers)\n",
        "\n",
        "for i in semantic_chunks[0:10]:\n",
        "  print(i, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTRRvU6DboO"
      },
      "source": [
        "### 4. Then we embed the chunked texts using a Transformer and create a Faiss Vector Database\n",
        "This allows us to encode the text into our search. Let's investigate the retrieved documents for a query.\n",
        "\n",
        "Vector databases, also called vector storage, efficiently store and retrieve vector data, which are arrays of numerical values representing points in multi-dimensional space. They're useful for handling data like embeddings from deep learning models or numerical features. Unlike traditional relational databases, which aren't optimized for vectors, vector databases offer efficient storage, indexing, and querying for high-dimensional and variable-length vectors.\n",
        "\n",
        "There are various types of vector databases:\n",
        "1. Chroma\n",
        "2. FAISS\n",
        "3. Pinecone\n",
        "4. Weaviate\n",
        "5. Qdrant\n",
        "\n",
        "Here, we build this using the FAISS utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fRCusd9royy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83aa296-ff7c-4fdb-d54e-252f02351688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experimentally characterized designs. J.W., A.L. and W.S. contributed additional code. S.O. implemented RFdiffusion on Google Colab.\n",
            "9h). Over the binder alone, the experimental structure deviates from the RFdiffusion design by only 0.6 (Fig. 6h). These results demonstrate the ability of RFdiffusion to generate new proteins with atomic level accuracy, and to precisely target functionally relevant sites on therapeutically important proteins. Discussion RFdiffusion is a comprehensive improvement over current protein design methods. RFdiffusion readily generates diverse uncondi- tional designs up to 600 residues in length that are accurately pre- dicted by AF2, far exceeding the complexity and accuracy achieved by most previous methods (a recent Hallucination-based approach also achieved high unconditional performance53). Half of our tested unconditional designs express in a soluble way, and have circular dichroism spectra consistent with the design models and high ther- mostability. Despite their substantially increased complexity, the ideality and stability of RFdiffusion designs is akin to that of de novo protein designs generated using previous methods such as Rosetta. RFdiffusion enables generation of higher-order architectures with any desired symmetry, unlike Hallucination methods, which have so far been limited to cyclic symmetries. Electron microscopy confirmed that the structures of these oligomers are very similar to the design mod- els, which in many cases show little global similarity to known protein oligomers. There has been recent progress in scaffolding protein functional motifs using deep-learning methods (RF Hallucination, RFjoint Inpainting and diffusion), but Hallucination is slow for large systems, Inpainting fails when insufficient starting information is provided and previous diffusion methods had low accuracy. RFdiffusion outperforms these previous methods in the complexity of the motifs that can be scaf- folded, the precision with which sidechains are positioned (for cataly- sis and other functions), and the accuracy of motif recapitulation by AF2. The design of MDM2 binding proteins with three orders of magni- tude higher affinities than the scaffolded P53 motif demonstrates the robustness of RFdiffusion motif scaffolding. Combining accurate motif scaffolding with the design of symmetric assemblies enabled consist- ent and atomically precise positioning of sidechains to coordinate Ni2+ ions across diverse tetrameric assemblies\n",
            "\n",
            "For binder design from target structural information alone, previous work required testing tens of thousands of sequences12. RFdiffusion, when combined with improved filtering26 raises experimental success rates by two orders of magnitude; high-affinity binders can be identi- fied from dozens of designs, in many cases eliminating the require- ment for slow and expensive high-throughput screening (at least for the non-polar sites targeted here; further studies will be required to assess success rates on more polar target sites and sites without native binding partners). A high-resolution cryo-EM structure of one of these designs in complex with influenza HA shows that RFdiffusion can design functional proteins with atomic accuracy. Vzquez Torres et al. demonstrate the ability of RFdiffusion to design picomolar affin- ity binders to flexible helical peptides54, further highlighting its use for de novo binder design. Vzquez Torres et al. also show how RFdiffusion\n",
            "\n",
            "can be extended for protein model refinement by partial noising and denoising, which enables tuneable sampling around a given input structure. For peptide binder design, this enabled increases in affin- ity of nearly three orders of magnitude without high-throughput screening. The breadth and complexity of problems solvable with RFdiffusion and the robustness and accuracy of the solutions far exceeds what has been achieved previously. In a manner reminiscent of the generation of images from text prompts, RFdiffusion makes possible, with mini- mal specialist knowledge, the generation of functional proteins from minimal molecular specifications (for example, high-affinity binders to a user-specified target protein, and diverse protein assemblies from user-specified symmetries). The power and scope of RFdiffusion can be extended in several directions. RF has recently been extended to nucleic acids and proteinnucleic acid complexes55, which should enable RFdiffusion to design nucleic acid binding proteins and perhaps folded RNA struc- tures. Extension of RF to incorporate ligands should similarly enable extension of RFdiffusion to explicitly model ligand atoms, and allow the design of proteinligand interactions. The ability to customize RFdif- fusion to specific design challenges by addition of external potentials and by fine-tuning (as illustrated here for catalytic site scaffolding, binder-targeting and fold specification), along with continued improve- ments to the underlying methodology, should enable de novo protein design to achieve still higher levels of complexity, to approach and, in some cases, surpass what natural evolution has achieved. Online content Any methods, additional references, Nature Portfolio reporting summa- ries, source data, extended data, supplementary information, acknowl- edgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-023-06415-8. 1. 2. Dauparas, J.\n",
            "One hundred designs were generated per problem, with no previous optimization on the benchmark set (some optimization was necessary for Hallucination). Supplementary Table 10 presents full results. In silico success rates on the problems are correlated between the methods, and RFdiffusion can still struggle on challenging problems in which all methods have low success. b, Four examples of designs in which RFdiffusion significantly outperforms existing methods. Teal, native motif; colours, AF2 prediction of a design. Metrics (r.m.s.d. AF2 versus design/versus native motif (), AF2 pAE): 5TRV long, 1.17/0.57; 4.73; 6E6R long, 0.89/0.27, 4.56; 7MRX long, 0.84/0.82 4.32; 5TPN, 0.59/0.49 3.77. c, RFdiffusion can scaffold the p53 helix that binds MDM2\n",
            "\n",
            "(left) and makes extra contacts with the target (right, average 31% increased surface area. Design was p53_design_89). Designs were generated with an RFdiffusion model fine-tuned on complexes. d, BLI measurements indicate high-affinity binding to MDM2 (p53_design_89, 0.7nM; p53_design_53, 0.5nM); the native affinity is 600nM (ref. 42). e, Out of 95 designs, 55 showed binding to MDM2 (more than 50% of maximum response). Thirty-two of these were monomeric (Supplementary Fig. 10h). f, After fine-tuning (Supplementary Methods), RFdiffusion can scaffold enzyme active sites. An oxidoreductase example (EC1) is shown (PDB 1A4I); catalytic site (teal); RFdiffusion output (grey, model; colours, AF2 prediction); zoom of active site. AF2 versus design backbone r.m.s.d. 0.88, AF2 versus design motif backbone r.m.s.d. 0.53, AF2 versus design motif full-atom r.m.s.d. 1.05, AF2 pAE 4.47. g, In silico success rates on active sites derived from EC1-5 (AF2 Motif r.m.s.d.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "faiss_vector_db = FAISS.from_documents(semantic_chunks, biomedical_encoder)\n",
        "question = \"Do you have any information on publications about RFDiffusion?\"\n",
        "searchDocs = faiss_vector_db.similarity_search(question)\n",
        "\n",
        "#investigate top-3 nearest (most relevant) documents for the query.\n",
        "print(searchDocs[0].page_content)\n",
        "print(searchDocs[1].page_content)\n",
        "print(searchDocs[2].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqccXGwoEYP-"
      },
      "source": [
        "![vector_database](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/vector_database.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/primer-on-vector-databases-and-retrieval-augmented-generation-rag-using-langchain-pinecone-37a27fb10546"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A56JddAEkCF"
      },
      "source": [
        "### 5. Initialize the LLM that will be used for question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdUokERIMnz3"
      },
      "source": [
        "Here, we use a pretrained model flan-t5-large as part of a HuggingFacePipeline. This will later be chained with the vector database for RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CXf3N6c134r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489700fe-c8a4-43be-80f6-97a65587da26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "   pipeline = pipe,\n",
        "   model_kwargs={\"temperature\": 0, \"max_length\": 2048, \"max_new_tokens\": 1024, \"device\":\"cuda\"},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJhhDzYEvhu"
      },
      "source": [
        "### 6. Retrieve data and use it to answer a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOQG22pHOf5W"
      },
      "source": [
        "![rag_workflow](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/rag_workflow.png?raw=1)\n",
        "\n",
        "Image credit: https://blog.gopenai.com/retrieval-augmented-generation-101-de05e5dc21ef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJHxBc9hzJB"
      },
      "source": [
        "Let's ask questions it would only be able to know if the model actually read the texts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMrN1_3B2cA1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"You are an honest and helpful AI. You are alwasys truthful and concise in your answers. Please answer the question with the provided context.\n",
        "If you don't know the answer, please say I don't know.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLCz3eiX2e0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4022d32-7ae6-42e6-f359-a2b52025c363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoseTTAFold diffusion\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=faiss_vector_db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "result = qa_chain ({ \"query\" : \"What technique proposed in 2023 can be used to predict protein folding?\" })\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkOkS39K2FT"
      },
      "source": [
        "Now let's ask the chain where to find the article related to RFDiffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKAT3UJU2DGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2399233e-d9ea-4aab-a947-09bc4d4ff52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Where was the RFdiffusion paper published?', 'result': 'Nature'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "qa_chain ({ \"query\" : \"Where was the RFdiffusion paper published?\" })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfO4a1s7ycai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59595646-385c-484e-d353-0c5f27e7f379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What can I use RFdiffusion model for?', 'result': \"I don't know\"}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "qa_chain ({ \"query\" : \"What can I use RFdiffusion model for?\" })"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}